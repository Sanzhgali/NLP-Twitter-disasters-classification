{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, This is my notebook for Twitter Disaster Classification competition on Kaggle. Link to the competition, where you can also find all files: https://www.kaggle.com/c/nlp-getting-started   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This competition helps to get into NLP (Natural Languange Processing)\n",
    "My work-flow:\n",
    "    1. Question (mission) of this project\n",
    "    2. Get and Clean Data\n",
    "    3. Perform Exploratory Data Analysis\n",
    "    4. Apply some NLP techniques\n",
    "    5. Share Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (task) of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the 'getting started' projects on Kaggle. Twitter is one of the most popular soc.media where people post their opinion/ news/ article.\n",
    "There are a lot of metaphorically used words to describe something.\n",
    "And task is to detect which post was about real catastrophical event and which was just metaphora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 files in this competition: training - for exploratory and tuning-model purpose; test - to test our model and submit into Kaggle; sample submission - how final data should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n",
    "\n",
    "# As we see our target variable is target column (1 for real disaster, 0 for not disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file shape: (7613, 5), test file shape : (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train file shape: {train.shape}, test file shape : {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['keyword'].unique()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Birmingham', 'Est. September 2012 - Bristol', 'AFRICA',\n",
       "       'Philadelphia, PA', 'London, UK', 'Pretoria', 'World Wide!!',\n",
       "       'Paranaque City', 'Live On Webcam'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['location'].unique()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we gain here: 4 attributues: Id (just to not mess up queue), Keyword: main word that described this tweet, Location: very unstructured data and Text: text of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values in Location and Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both train and test contains NA values so let's fill them\n",
    "\n",
    "def fillna_column(column, imputer):\n",
    "    if column.isnull().sum() > 0:\n",
    "        return column.fillna(imputer)\n",
    "    \n",
    "    else:\n",
    "        return column\n",
    "    \n",
    "train['location'] = fillna_column(train['location'], 'Unknown')\n",
    "train['keyword'] = fillna_column(train['keyword'], 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test also\n",
    "test['location'] = fillna_column(test['location'], 'Unknown')\n",
    "test['keyword'] = fillna_column(test['keyword'], 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will drop Location column as it is hard to structure and gain some information from this attribute\n",
    "\n",
    "train.drop('location', axis = 1, inplace = True)\n",
    "test.drop('location', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accident center lane blocked in #SantaClara on US-101 NB before Great America Pkwy #BayArea #Traffic http://t.co/pmlOhZuRWR'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[69, 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "    1. Lowercase all words\n",
    "    2. Remove punctuation\n",
    "    3. Remove numbers\n",
    "    4. Remove links\n",
    "    5. Remove meaningless words like (like, e.t.c, then, by...)\n",
    "    6. Tokenize words\n",
    "    7. Remove stop words or most common words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 1: CLeaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    \n",
    "    \n",
    "    text = text.lower() # Lower case text\n",
    "    \n",
    "    text = re.sub('@\\w*:.', '', text) # remove account names\n",
    "    \n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) # remove links\n",
    "   \n",
    "\n",
    "    text = ' '.join(s for s in text.split() if not any(c.isdigit() for c in s)) # remove words containing digits\n",
    "    \n",
    "\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text) # remove special characters\n",
    "    \n",
    "\n",
    "    text = text.replace('  ', ' ') # remove extra space\n",
    "    \n",
    "    text = text.strip() # Remove extra space from beginning and ending of text\n",
    "    \n",
    "    return text\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(clean_text_round1)\n",
    "\n",
    "test['text'] = test['text'].apply(clean_text_round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean keyword cloumn: we see here %20 instead of space so let's replace it\n",
    "\n",
    "def remove_spec_chars(example):\n",
    "\n",
    "    return re.sub('[^A-Za-z ]+', ' ', example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'] = train['keyword'].apply(remove_spec_chars)\n",
    "test['keyword'] = test['keyword'].apply(remove_spec_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['keyword'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, I want to divide keywords to classes and categoralize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_related = ['airplane accident', 'ambulance', 'army', 'arson', 'arsonist', 'attack', 'attacked', 'battle', 'bioterror', 'bioterrorism', 'bleeding', \n",
    "                 'blood', 'bloody', 'blown up', 'body bag', 'body bagging', 'body bags', 'bomb', 'bombed', 'bombing', 'bridge collapse',\n",
    "                 'buildings burning', 'buildings on fire', 'burned', 'burning', 'burning buildings', 'casualties', 'casualty', 'chemical emergency', 'crash', 'crashed', 'crush', 'crushed', \n",
    "                 'curfew', 'dead', 'death', 'deaths', 'debris', 'demolish', 'demolished', 'demolition', 'derail',\n",
    "                 'derailed', 'derailment', 'desolate', 'desolation', 'destroy',\n",
    "                 'destroyed', 'destruction', 'detonate', 'detonation', 'devastated', 'devastation', 'drown',\n",
    "                 'drowned', 'drowning', 'electrocute', 'electrocuted', 'emergency', 'emergency plan', 'emergency services',\n",
    "                 'engulfed', 'explode', 'exploded', 'explosion',  'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear',\n",
    "                 'fire truck', 'first responders', 'flames', 'flattened', 'harm', 'hijack', 'hijacker', 'hijacking', 'hostage',\n",
    "                 'hostages', 'injured', 'injuries', 'injury',  'mass murder', 'mass murderer', 'massacre', 'mayhem', 'military',\n",
    "                 'nuclear disaster', 'nuclear reactor', 'oil spill', 'outbreak', 'panic',\n",
    "                 'panicking', 'police', 'quarantine', 'quarantined', 'radiation emergency', 'razed', 'refugees', 'rescue',\n",
    "                 'rescued', 'rescuers', 'riot', 'rioting', 'ruin', 'screamed', 'screaming', 'screams', 'sinking', 'siren', 'sirens', 'smoke',\n",
    "                 'stretcher', 'structural failure', 'suicide bomb',\n",
    "                 'suicide bomber', 'suicide bombing', 'sunk', 'survive', 'survived', 'survivors', 'terrorism', 'terrorist', 'threat',\n",
    "                 'trapped', 'trauma', 'traumatised', 'trouble', 'upheaval', 'war zone', 'weapon', 'weapons',  'wounded', 'wounds',  'wreck', 'wreckage', 'wrecked',\n",
    "                ]\n",
    "all_categories = train['keyword'].unique()\n",
    "\n",
    "nature_related = list(set(all_categories) - set(human_related) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now' let's replace values with two values: Natural, Human\n",
    "train['keyword'] = train['keyword'].apply(lambda x: 'human' if x in human_related else 'nature')\n",
    "\n",
    "test['keyword'] = test['keyword'].apply(lambda x: 'human' if x in human_related else 'nature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Now let's use one-hot encoding for this column\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "\n",
    "keyword = train['keyword'].values\n",
    "\n",
    "keyword = keyword.reshape(-1, 1)\n",
    "\n",
    "keyword = enc.fit_transform(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>human</th>\n",
       "      <th>nature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>ariaahrary thetawniest the out of control wild...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>s of volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>police investigating after an ebike collided w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>the latest more homes razed by northern califo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  target  human  \\\n",
       "0         1  our deeds are the reason of this earthquake ma...       1    0.0   \n",
       "1         4              forest fire near la ronge sask canada       1    0.0   \n",
       "2         5  all residents asked to shelter in place are be...       1    0.0   \n",
       "3         6  people receive wildfires evacuation orders in ...       1    0.0   \n",
       "4         7  just got sent this photo from ruby alaska as s...       1    0.0   \n",
       "...     ...                                                ...     ...    ...   \n",
       "7608  10869  two giant cranes holding a bridge collapse int...       1    0.0   \n",
       "7609  10870  ariaahrary thetawniest the out of control wild...       1    0.0   \n",
       "7610  10871                                s of volcano hawaii       1    0.0   \n",
       "7611  10872  police investigating after an ebike collided w...       1    0.0   \n",
       "7612  10873  the latest more homes razed by northern califo...       1    0.0   \n",
       "\n",
       "      nature  \n",
       "0        1.0  \n",
       "1        1.0  \n",
       "2        1.0  \n",
       "3        1.0  \n",
       "4        1.0  \n",
       "...      ...  \n",
       "7608     1.0  \n",
       "7609     1.0  \n",
       "7610     1.0  \n",
       "7611     1.0  \n",
       "7612     1.0  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train, pd.DataFrame(keyword.toarray())], axis = 1)\n",
    "\n",
    "train = train.rename(columns = {0: 'human', 1: 'nature'})\n",
    "\n",
    "train.drop('keyword', axis = 1, inplace = True)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for test dataframe\n",
    "test = pd.concat([test, pd.DataFrame(enc.transform(test['keyword'].values.reshape(-1, 1)).toarray())], axis = 1)\n",
    "\n",
    "test = test.rename(columns = {0: 'human', 1: 'nature'})\n",
    "\n",
    "test.drop('keyword', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need two types of each dataset. \n",
    "1) Corpus - our collection of text in order\n",
    "2) Document-Term matrix - count appereance of word in row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already created a corpys so let's save datasets for other notebooks\n",
    "\n",
    "train.to_pickle('train_corpus.pkl')\n",
    "\n",
    "test.to_pickle('test_corpus.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documnet-Term matrix\n",
    "\n",
    "For many of the techniques we'll be using in future notebooks, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n",
    "\n",
    "In addition, with CountVectorizer, we can remove stop words. Stop words are common words that add no additional meaning to text such as 'a', 'the', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "train_cv = cv.fit_transform(train['text'])\n",
    "\n",
    "train_dtm = pd.DataFrame(train_cv.toarray(), columns = cv.get_feature_names())\n",
    "\n",
    "train_dtm.index = train.index\n",
    "\n",
    "train_dtm['id'] = train['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaaallll</th>\n",
       "      <th>aaaaaand</th>\n",
       "      <th>aaarrrgghhh</th>\n",
       "      <th>aaceorg</th>\n",
       "      <th>aampb</th>\n",
       "      <th>aampw</th>\n",
       "      <th>aan</th>\n",
       "      <th>aannnnd</th>\n",
       "      <th>...</th>\n",
       "      <th>zonesthank</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouma</th>\n",
       "      <th>zourryart</th>\n",
       "      <th>zrnf</th>\n",
       "      <th>zss</th>\n",
       "      <th>zumiez</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zxathetis</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 15993 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aaaa  aaaaaaallll  aaaaaand  aaarrrgghhh  aaceorg  aampb  aampw  \\\n",
       "0      0     0            0         0            0        0      0      0   \n",
       "1      0     0            0         0            0        0      0      0   \n",
       "2      0     0            0         0            0        0      0      0   \n",
       "3      0     0            0         0            0        0      0      0   \n",
       "4      0     0            0         0            0        0      0      0   \n",
       "...   ..   ...          ...       ...          ...      ...    ...    ...   \n",
       "3258   0     0            0         0            0        0      0      0   \n",
       "3259   0     0            0         0            0        0      0      0   \n",
       "3260   0     0            0         0            0        0      0      0   \n",
       "3261   0     0            0         0            0        0      0      0   \n",
       "3262   0     0            0         0            0        0      0      0   \n",
       "\n",
       "      aan  aannnnd  ...  zonesthank  zoom  zouma  zourryart  zrnf  zss  \\\n",
       "0       0        0  ...           0     0      0          0     0    0   \n",
       "1       0        0  ...           0     0      0          0     0    0   \n",
       "2       0        0  ...           0     0      0          0     0    0   \n",
       "3       0        0  ...           0     0      0          0     0    0   \n",
       "4       0        0  ...           0     0      0          0     0    0   \n",
       "...   ...      ...  ...         ...   ...    ...        ...   ...  ...   \n",
       "3258    0        0  ...           0     0      0          0     0    0   \n",
       "3259    0        0  ...           0     0      0          0     0    0   \n",
       "3260    0        0  ...           0     0      0          0     0    0   \n",
       "3261    0        0  ...           0     0      0          0     0    0   \n",
       "3262    0        0  ...           0     0      0          0     0    0   \n",
       "\n",
       "      zumiez  zurich  zxathetis  zzzz  \n",
       "0          0       0          0     0  \n",
       "1          0       0          0     0  \n",
       "2          0       0          0     0  \n",
       "3          0       0          0     0  \n",
       "4          0       0          0     0  \n",
       "...      ...     ...        ...   ...  \n",
       "3258       0       0          0     0  \n",
       "3259       0       0          0     0  \n",
       "3260       0       0          0     0  \n",
       "3261       0       0          0     0  \n",
       "3262       0       0          0     0  \n",
       "\n",
       "[3263 rows x 15993 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same goes for test\n",
    "\n",
    "test_cv = cv.transform(test['text'])\n",
    "\n",
    "test_dtm = pd.DataFrame(test_cv.toarray(), columns = cv.get_feature_names())\n",
    "\n",
    "test_dtm.index = test.index\n",
    "\n",
    "test_dtm['id'] = test['id']\n",
    "\n",
    "\n",
    "test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this save this datasets\n",
    "test_dtm.to_pickle('test_dtm.pkl')\n",
    "\n",
    "train_dtm.to_pickle('train_dtm.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning step is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
